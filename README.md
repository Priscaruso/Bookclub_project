# Book Club Project üìö 

üáßüá∑

A Book Club √© uma startup fict√≠cia de troca de livros, cujo modelo de neg√≥cio funciona com base na troca de livros pelos usu√°rios. Cada livro cadastrado pelo 
usu√°rio d√° direito √† uma troca, mas o usu√°rio tamb√©m pode comprar o livro, caso ele n√£o queira oferecer outro livro em troca. 

Uma das ferramentas mais importantes para que esse modelo de neg√≥cio rentabilize, √© a recomenda√ß√£o. Uma excelente recomenda√ß√£o aumenta o volume de 
trocas e vendas no site. No entanto, a empresa n√£o coleta e nem armazena os livros enviados pelos usu√°rios em um banco de dados. Os livros s√£o apenas enviados pelos usu√°rios atrav√©s de um bot√£o "Fazer Upload", ficando vis√≠veis na plataforma, junto com suas estrelas, que representam o quanto os usu√°rios gostaram do livro.

## T√≥picos do projeto

:small_blue_diamond: [Objetivo](#objetivo)

:small_blue_diamond: [Arquitetura dos dados](#arquitetura-dos-dados)

:small_blue_diamond: [Pr√©-requisitos](#pr√©-requisitos)

:small_blue_diamond: [Fases do Projeto](#fases-do-projeto)

:small_blue_diamond: [Pr√≥ximos passos](#pr√≥ximos-passos)


## Objetivo
Criar uma solu√ß√£o para coletar, armazenar e analisar os dados da plataforma web fict√≠cia de troca de livros usando os servi√ßos da nuvem AWS.

Dados desejados:

* Nome do livro
* Categoria do livro
* N√∫mero de estrelas
* Pre√ßo
* Disponibilidade do livro: em estoque ou n√£o

Link da plataforma: http://books.toscrape.com


## Arquitetura dos dados
A arquitetura do projeto utiliza os servi√ßos da nuvem AWS aplicados √† engenharia de dados, conforme mostra a figura a seguir:

![Arquitetura dos dados](https://user-images.githubusercontent.com/83982164/235518227-0165070a-5b47-459f-b75b-dd99b56203b3.png)



## Pr√©-requisitos
Para executar esse projeto √© necess√°rio criar um ambiente virtual, instalar os pacotes Python contidos no arquivo [requirements.txt](https://github.com/Priscaruso/Bookclub_project/blob/main/requirements.txt) e ter uma conta na [AWS](https://aws.amazon.com/pt/console/).

Esse projeto est√° dividido em fases de constru√ß√£o.

## Fases do Projeto

### 1¬™ fase - Coleta dos dados

Cria√ß√£o do script python [bookclub_webscraper.py](https://github.com/Priscaruso/Bookclub_project/blob/main/bookclub_webscraper.py) que define as fun√ß√µes para a coleta dos dados da plataforma.

O funcionamento do script acontece da seguinte forma:
* Fun√ß√£o _"get_book_links"_ navega por todas as p√°ginas da plataforma, coletando e armazenando os links da p√°gina de cada livro encontrado dentro de uma lista usando as bibliotecas Selenium e BeautifulSoup 
* Fun√ß√£o _"get_book_data"_ navega por cada p√°gina do livro para coletar os dados desejados e armazen√°-los dentro de uma lista


### 2¬™ fase - Armazenamento dos dados

Essa fase √© subdividida em 3 passos:
* Cria√ß√£o do banco de dados

  Constru√ß√£o de uma inst√¢ncia RDS PostgreSQL na nuvem AWS para armazenar os dados operacionais da plataforma, conforme mostra a figura abaixo: 
  
  ![Banco de dados RDS](https://github.com/Priscaruso/Bookclub_project/assets/83982164/3212367c-f903-4a63-bef8-c0bcbb9338b7)

   A regi√£o selecionada foi a de Oregon nos EUA (us-west-2) e a m√°quina foi a db.t3.micro por quest√µes de custo e para atender ao volume de dados necess√°rio para esse projeto.
 
* Modelagem dos dados

  Cria√ß√£o do script python [data_model.py] que cria as seguintes fun√ß√µes:
  * Fun√ß√£o _check_if_valid_data_ que verifica se os dados coletados apresentam dados ausentes ou n√£o
  * Fun√ß√£o _connect_db_ que cria uma conex√£o com o banco bookclub no RDS usando a url do host, o nome do banco, o usu√°rio padr√£o do banco, a senha de acesso criada e a porta de conex√£o. Tanto o usu√°rio como a senha de acesso s√£o armazenados em um arquivo .env no ambiente virtual criado para o projeto, por meio da biblioteca python dotenv, a fim de garantir a seguran√ßa dos dados.
  * Fun√ß√£o _create_table_ que gera a tabela desejada a partir da execu√ß√£o de uma query, sendo esta tabela armazenada dentro do banco bookclub no RDS. O schema da tabela √© apresentado a seguir:
    
    | Column | Data Type |
    | :---: | :---: |
    | id | int autoincremental sequence, PRIMARY KEY |
    | name | varchar(250) |
    | category | varchar(20) |
    | stars | varchar(5) |
    | price | float |
    | availability | varchar(10) |
    
  
 
* Inser√ß√£o dos dados no banco

  Dentro do script python [data_model.py]() s√£o criadas as seguintes fun√ß√µes:
    * Fun√ß√£o _insert_data_ para inserir os dados no banco a partir de uma query e dos valores das vari√°veis armazenados por meio de uma tupla
    * Fun√ß√£o _consulta_db_ que permite realizar consulta dos dados no banco por meio de uma query e retorna os dados na forma de lista

Ap√≥s a cria√ß√£o das fun√ß√µes, √© configurado os par√¢metros de navega√ß√£o para a biblioteca selenium, √© criado o objeto de navega√ß√£o, √© chamada a fun√ß√£o _get_book_links_ e _get_book_data_ para coletar os dados da plataforma. Os dados s√£o transformados em dataframe, √© chamada a fun√ß√£o _create_table_ para criar a tabela books dentro do RDS, s√£o realizadas as transforma√ß√µes necess√°rias nos dados (remo√ß√£o de ap√≥strofes e sinais de % em strings) e feita a inser√ß√£o desses dados na tabela books dentro do banco bookclub no RDS.


### 3¬™ fase - Constru√ß√£o do Datalake
Nesta fase √© criado o Datalake usando o Amazon S3, que √© um reposit√≥rio de objetos. 
S√£o criadas tr√™s camadas para armazenar os dados:
  * raw-bookclub - √© a camada raw, tamb√©m conhecida como bronze, onde ficam os dados brutos coletados no seu formato original (geralmente CSV)
  
     ![bucket raw](https://github.com/Priscaruso/Bookclub_project/assets/83982164/e9f615da-84ab-4862-8009-c3b3974541a9)

  
  * processed-bookclub  - √© a camada processed, tamb√©m conhecida como silver, onde ficam os dados que foram transformados para o formato delta, mais otimizado para serem usados na nuvem pelos cientistas e analistas de dados
  
    ![bucket processed](https://github.com/Priscaruso/Bookclub_project/assets/83982164/80bec75d-8486-4ae6-9f6a-9de3302586b8)

  * curated-bookclub - √© a camada curated, tamb√©m conhecida como gold, onde ficam armazenadas as tabelas anal√≠ticas, tamb√©m em formato delta, j√° transformadas conforme os requisitos da √°rea de neg√≥cio, para serem usadas pelos analistas de neg√≥cios
  
    ![bucket curated](https://github.com/Priscaruso/Bookclub_project/assets/83982164/8c53685d-6385-4d03-b157-7a1c0f0c62f4)


### 4¬™ fase - Migra√ß√£o dos dados
Para que as transforma√ß√µes e an√°lises dos dados possam ser realizadas, os dados necessitam estar armazenados em um local apropriado
para isso e que tenha condi√ß√µes de receber constantemente novos dados, ou seja, seja escal√°vel. Assim, o DataLake √© o local ideal
para realizar o armazenamento desses dados. Para que isso seja poss√≠vel, √© preciso realizar a migra√ß√£o dos dados brutos contidos no RDS para a camada raw do DataLake (bucket raw-bookclub). Nesse projeto √© usado o Database Migration Service (DMS) da AWS, que pode realizar a migra√ß√£o de banco de dados do ambiente on-premise para a nuvem ou da pr√≥pria nuvem para outro local na nuvem, gerenciar a migra√ß√£o tanto de dados full como de dados incrementais por meio do Change Data Capture (CDC). Tem um custo razo√°vel, √© f√°cil de usar e realiza a migra√ß√£o de forma mais r√°pida. A migra√ß√£o √© feita de acordo com os seguintes passos:
  * Criar uma inst√¢ncia de replica√ß√£o no DMS

    A inst√¢ncia √© criada com o nome _dms-instance-01_ na mesma regi√£o dos demais servi√ßos, Oregon (us-west-2), com uma engine na vers√£o 3.4.6, uma m√°quina dms.t3.micro, que tem uma mem√≥ria menor, e no ambiente de dev/teste. O ideal √© selecionar uma m√©moria que corresponda, no m√≠nimo, ao tamanho de mem√≥ria do banco de dados de origem para que n√£o haja problemas de mem√≥ria. Em rela√ß√£o ao espa√ßo de armazenamento, deve ter no m√≠nimo metade do tamanho do banco de dados de origem. Essa inst√¢ncia √© respons√°vel pelo gerenciamento da tarefa de migra√ß√£o.
    
    ![inst√¢ncia de replica√ß√£o DMS](https://github.com/Priscaruso/Bookclub_project/assets/83982164/bf598a55-b25e-4fc4-a981-487922842d28)

  * Criar os endpoints de origem (rds-source-postgresql), que conecta o DMS com o RDS, e de destino (s3-target-datalake), que conecta o DMS com o Datalake S3
  
    ![endpoints](https://github.com/Priscaruso/Bookclub_project/assets/83982164/aa964165-ecc1-4d90-bdf7-94e1937d49b3)

  
  * Testar os endspoints para verificar se a conex√£o entre o RDS, o DMS e o S3 est√° funcionando corretamente
    
  
  * Criar uma tarefa de migra√ß√£o (task-01) usando a inst√¢ncia de replica√ß√£o e os endpoints gerados
  
    A tarefa √© respons√°vel por fazer a conex√£o entre os endpoints, ou seja, o banco de dados de origem e de destino, e √© executada pela inst√¢ncia de replica√ß√£o.
  
  ![tarefa de migra√ß√£o parte 2](https://github.com/Priscaruso/Bookclub_project/assets/83982164/5ba7ca5c-df11-4a1d-b0dc-bbce689581b1)
    
  Ap√≥s a conclus√£o da tarefa de migra√ß√£o, os dados ter√£o sido movidos para o bucket raw-bookclub do Datalake.


  ### 5¬™ fase - Constru√ß√£o do Data Warehouse 
O Data Warehouse √© o armaz√©m de dados anal√≠ticos, onde os analistas de neg√≥cios conseguem obter insights por meio de consultas SQL e tamb√©m servir os dados para as ferramentas anal√≠ticas de visualiza√ß√£o gerarem dashboards, que permitem a eles tomar melhores decis√µes. O Data Warehouse utilizado nesse projeto √© o Amazon Redshift, uma das solu√ß√µes mais usadas no mercado, que armazena as tabelas anal√≠ticas geradas de acordo com as regras de neg√≥cios na etapa de processamento dos dados. O Redshift possui um cluster com v√°rios n√≥s (m√°quinas), o que permite escalar a performance das consultas, al√©m de possuir diversos outros recursos para escabilidade e alta performance.
Para construir o Data Warehouse, precisa-se criar um cluster, conforme os passos abaixo:
  * definir o nome do cluster, no caso redshift-cluster-1
  * selecionar um n√≥ (servidor) do tipo dc2.large (n√≠vel gratuito/free-tier)
  * definir um usu√°rio e uma senha para acesso ao banco de dados que ser√° criado dentro do cluster
A figura abaixo mostra o cluster gerado:

  ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/bb0494fb-2889-4dcf-8890-727ee2771694)

O acesso ao Redshift se d√° atrav√©s do editor de consultas v2 (query editor v2) no qual pode-se realizar consultas SQL nas tabelas anal√≠ticas geradas atrav√©s da aplica√ß√£o pyspark no processamento dos dados.


### 6¬™ fase - Processamento dos dados
Nesta etapa foi utilizado o EMR (Elastic Map Reduce) da AWS para realizar o processamento dos dados, usando uma aplica√ß√£o Spark, que possibilita o processamento de grande volume de dados de forma mais eficiente. A op√ß√£o por usar o EMR, √© que ele √© um cluster (uma m√°quina EC2), que pode vir instalado com diversas aplica√ß√µes open source do ecossistema Hadoop e tem as bibliotecas necess√°rias (como as para o formato Delta) e os JARS necess√°rios para trabalhar com Redshift j√° instalados, o que facilita e economiza tempo. Al√©m disso, ele s√≥ cobra pelo tempo de uso da m√°quina, podendo processar a quantidade de dados que desejar nesse per√≠odo, sem ter aumento de custo por conta disso. 
O processamento dos dados consiste nos seguintes passos:
  * Criar um cluster EMR contendo somente a aplica√ß√£o Spark vers√£o 3.3.0
    
    Foi usado o EMR vers√£o 6.9.0 que j√° vem com as bibliotecas Delta e m√°quinas EC2 do tipo m4.large, que tem menor custo e menor recurso computacional, mas atende ao objetivo do projeto. Durante a cria√ß√£o do cluster, √© necess√°rio gerar um par de chaves SSH no formato PEM para acessar as m√°quinas EC2 do cluster EMR. Al√©m disso, tamb√©m √© necess√°rio criar as roles de acesso a servi√ßos da AWS (EMR_DefaultRole_V2), que permitem o acesso do EMR ao S3 para armazenar os dados processados, e de acesso a inst√¢ncias EC2 (EMR_EC2_DefaultRole), para provisionar as m√°quinas do cluster.
  
    ![cluster EMR - vers√£o editada](https://github.com/Priscaruso/Bookclub_project/assets/83982164/89a21f7b-3fcf-4ef6-99a4-f2888e5f1515)

  * Criar um job spark por meio de uma aplica√ß√£o pyspark
  
    O job spark √© uma tarefa que ser√° executada pelo cluster EMR, no caso, a aplica√ß√£o pyspark criada. A aplica√ß√£o √© respons√°vel por fazer as transforma√ß√µes desejadas nos dados e inser√≠-los na camada processed (bucket processed-bookclub). Ela tamb√©m gera as tabelas anal√≠ticas conforme os requisitos solicitados pela √°rea de neg√≥cios e carrega-as tanto na camada curated (bucket curated-bookclub) do datalake como no Data Warehouse, que √© o Amazon Redshift. Essa aplica√ß√£o pyspark de nome [job_spark_app_emr_redshift.py](https://github.com/Priscaruso/Bookclub_project/blob/main/processing/job_spark_app_emr_redshift.py) pode ser encontrada dentro da pasta processing deste reposit√≥rio.
   
Para executar o job spark necessita-se:
  * habilitar a permiss√£o de execu√ß√£o para o propriet√°rio do arquivo do par de chaves SSH: `chmod 400 local_das_chaves`, onde local_das_chaves √© a pasta onde o par de chaves SSH foi salvo
  * mover a aplica√ß√£o criada para dentro do diret√≥rio padr√£o do Hadoop no cluster EMR, conforme o comando `scp -i local_das_chaves job-spark-app-emr-redshift.py hadoop@url_do_servidor:/home/hadoop/`, onde url_do_servidor √© o link do DNS p√∫blico do n√≥ prim√°rio (servidor master do EMR) localizado na console da AWS a partir das informa√ß√µes do cluster-bookclub
  * conectar remotamente no servidor master usando ssh: `ssh -i local_das_chaves hadoop@url_do_servidor`
  * executar o comando spark-submit para rodar a aplica√ß√£o:
    
    `spark-submit --packages io.delta:delta-core_2.12:2.0.0 --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"  --jars /usr/share/aws/redshift/jdbc/RedshiftJDBC.jar,/usr/share/aws/redshift/spark-redshift/lib/spark-redshift.jar,/usr/share/aws/redshift/spark-redshift/lib/spark-avro.jar,/usr/share/aws/redshift/spark-redshift/lib/minimal-json.jar job-spark-app-emr-redshift.py`
    
O comando acima faz a configura√ß√£o das bibliotecas Delta, para gerar os dados nesse formato, e JARS, para o EMR conseguir trabalhar junto com o Redshift, executando logo em seguida, a aplica√ß√£o.
Ao concluir a execu√ß√£o da aplica√ß√£o, os dados transformados em formato delta estar√£o localizados no bucket processed-bookclub e as tabelas anal√≠ticas _top10_liked_books_ e _top10_prices_ geradas a partir deles, no bucket curated-bookclub e no Redshift.


### 7¬™ fase - Consulta dos dados 
Essa etapa consiste no acesso aos dados anal√≠ticos j√° transformados, de acordo com as regras de neg√≥cios definidas, por meio do Amazon Athena. O Athena √© um servi√ßo serverless (sem necessidade de provisionar servidor) de consultas ad-hoc simples, onde os analistas de neg√≥cios podem rapidamente realizar consultas interativas de forma simples para obter os insights desejados a partir dos dados tratados que est√£o armazenados no S3, no caso desse projeto s√£o os que se encontram na camada curated do Datalake, o bucket curated-bookclub. O custo dele √© por consulta, por quantidade de terabyte escaneado, assim o ideal √© tentar definir um limite usando workgroups (grupos de trabalho) para obter os insights desejados utilizando o menor n√∫mero de consultas poss√≠vel. Para acessar esses dados no Athena, primeiramente, devem ser realizados os seguintes passos:

  * Criar um crawler no Glue
    O crawler vai escanear e inferir automaticamente o schema dos dados transformados armazenados no bucket curated-bookclub do S3.
    
  * criar uma database no Glue Data Catalog
    A database √© como se fosse um banco de dados, mas de fato n√£o √©, 

### 8¬™ fase - Visualiza√ß√£o dos dados

## Problemas encontrados

## Pr√≥ximos passos
Com o desejo de evoluir o projeto e torn√°-lo ainda mais completo, quero incluir nos pr√≥ximos meses as seguintes funcionalidades:
  * Orquestrar o pipeline usando o Airflow
  * Criar toda infraestrutura como c√≥digo (IaC) usando o Terraform

------------------------------------------------------------------------------------------------------



