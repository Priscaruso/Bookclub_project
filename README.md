# Book Club Project üìö 

üáßüá∑

A Book Club √© uma startup fict√≠cia de troca de livros, cujo modelo de neg√≥cio funciona com base na troca de livros pelos usu√°rios. Cada livro cadastrado pelo 
usu√°rio d√° direito √† uma troca, mas o usu√°rio tamb√©m pode comprar o livro, caso ele n√£o queira oferecer outro livro em troca. 

Uma das ferramentas mais importantes para que esse modelo de neg√≥cio rentabilize, √© a recomenda√ß√£o. Uma excelente recomenda√ß√£o aumenta o volume de 
trocas e vendas no site. No entanto, a empresa n√£o coleta e nem armazena os livros enviados pelos usu√°rios em um banco de dados. Os livros s√£o apenas enviados pelos usu√°rios atrav√©s de um bot√£o "Fazer Upload", ficando vis√≠veis na plataforma, junto com suas estrelas, que representam o quanto os usu√°rios gostaram do livro.

## T√≥picos do projeto

:small_blue_diamond: [Objetivo](#objetivo)

:small_blue_diamond: [Arquitetura dos dados](#arquitetura-dos-dados)

:small_blue_diamond: [Pr√©-requisitos](#pr√©-requisitos)

:small_blue_diamond: [Fases do Projeto](#fases-do-projeto)

:small_blue_diamond: [Pr√≥ximos passos](#pr√≥ximos-passos)


## Objetivo
Criar uma solu√ß√£o para coletar, armazenar e analisar os dados da plataforma web fict√≠cia de troca de livros usando os servi√ßos da nuvem AWS.

Dados desejados:

* Nome do livro
* Categoria do livro
* N√∫mero de estrelas
* Pre√ßo
* Disponibilidade do livro: em estoque ou n√£o

Link da plataforma: http://books.toscrape.com


## Arquitetura dos dados
A arquitetura do projeto utiliza os servi√ßos da nuvem AWS aplicados √† engenharia de dados, conforme mostra a figura a seguir:

![Arquitetura dos dados](https://user-images.githubusercontent.com/83982164/235518227-0165070a-5b47-459f-b75b-dd99b56203b3.png)



## Pr√©-requisitos
Para executar esse projeto √© necess√°rio criar um ambiente virtual, instalar os pacotes Python contidos no arquivo [requirements.txt](https://github.com/Priscaruso/Bookclub_project/blob/main/requirements.txt) e ter uma conta na [AWS](https://aws.amazon.com/pt/console/).

Esse projeto est√° dividido em fases de constru√ß√£o.

## Fases do Projeto

### 1¬™ fase - Coleta dos dados

Cria√ß√£o do script python [bookclub_webscraper.py](https://github.com/Priscaruso/Bookclub_project/blob/main/bookclub_webscraper.py) que define as fun√ß√µes para a coleta dos dados da plataforma.

O funcionamento do script acontece da seguinte forma:
* Fun√ß√£o _"get_book_links"_ navega por todas as p√°ginas da plataforma, coletando e armazenando os links da p√°gina de cada livro encontrado dentro de uma lista usando as bibliotecas Selenium e BeautifulSoup 
* Fun√ß√£o _"get_book_data"_ navega por cada p√°gina do livro para coletar os dados desejados e armazen√°-los dentro de uma lista


### 2¬™ fase - Armazenamento dos dados

Essa fase √© subdividida em 3 passos:
* Cria√ß√£o do banco de dados

  Constru√ß√£o de uma inst√¢ncia RDS PostgreSQL na nuvem AWS para armazenar os dados operacionais da plataforma, conforme mostra a figura abaixo: 
  
  ![Banco de dados RDS](https://github.com/Priscaruso/Bookclub_project/assets/83982164/3212367c-f903-4a63-bef8-c0bcbb9338b7)

   A regi√£o selecionada foi a de Oregon nos EUA (us-west-2) e a m√°quina foi a db.t3.micro por quest√µes de custo e para atender ao volume de dados necess√°rio para esse projeto.
 
* Modelagem dos dados

  Cria√ß√£o do script python [data_model.py] que cria as seguintes fun√ß√µes:
  * Fun√ß√£o _check_if_valid_data_ que verifica se os dados coletados apresentam dados ausentes ou n√£o
  * Fun√ß√£o _connect_db_ que cria uma conex√£o com o banco bookclub no RDS usando a url do host, o nome do banco, o usu√°rio padr√£o do banco, a senha de acesso criada e a porta de conex√£o. Tanto o usu√°rio como a senha de acesso s√£o armazenados em um arquivo .env no ambiente virtual criado para o projeto, por meio da biblioteca python dotenv, a fim de garantir a seguran√ßa dos dados.
  * Fun√ß√£o _create_table_ que gera a tabela desejada a partir da execu√ß√£o de uma query, sendo esta tabela armazenada dentro do banco bookclub no RDS. O schema da tabela √© apresentado a seguir:
    
    | Column | Data Type |
    | :---: | :---: |
    | id | int autoincremental sequence, PRIMARY KEY |
    | name | varchar(250) |
    | category | varchar(20) |
    | stars | varchar(5) |
    | price | float |
    | availability | varchar(10) |
    
  
 
* Inser√ß√£o dos dados no banco

  Dentro do script python [data_model.py]() s√£o criadas as seguintes fun√ß√µes:
    * Fun√ß√£o _insert_data_ para inserir os dados no banco a partir de uma query e dos valores das vari√°veis armazenados por meio de uma tupla
    * Fun√ß√£o _consulta_db_ que permite realizar consulta dos dados no banco por meio de uma query e retorna os dados na forma de lista

Ap√≥s a cria√ß√£o das fun√ß√µes, √© configurado os par√¢metros de navega√ß√£o para a biblioteca selenium, √© criado o objeto de navega√ß√£o, √© chamada a fun√ß√£o _get_book_links_ e _get_book_data_ para coletar os dados da plataforma. Os dados s√£o transformados em dataframe, √© chamada a fun√ß√£o _create_table_ para criar a tabela books dentro do RDS, s√£o realizadas as transforma√ß√µes necess√°rias nos dados (remo√ß√£o de ap√≥strofes e sinais de % em strings) e feita a inser√ß√£o desses dados na tabela books dentro do banco bookclub no RDS.


### 3¬™ fase - Constru√ß√£o do Datalake
Nesta fase √© criado o Datalake usando o Amazon S3, que √© um reposit√≥rio de objetos. 
S√£o criadas tr√™s camadas para armazenar os dados:
  * raw-bookclub - √© a camada raw, tamb√©m conhecida como bronze, onde ficam os dados brutos coletados no seu formato original (geralmente CSV)
  
     ![bucket raw](https://github.com/Priscaruso/Bookclub_project/assets/83982164/e9f615da-84ab-4862-8009-c3b3974541a9)

  
  * processed-bookclub  - √© a camada processed, tamb√©m conhecida como silver, onde ficam os dados que foram transformados para o formato delta, mais otimizado para serem usados na nuvem pelos cientistas e analistas de dados
  
    ![bucket processed](https://github.com/Priscaruso/Bookclub_project/assets/83982164/80bec75d-8486-4ae6-9f6a-9de3302586b8)

  * curated-bookclub - √© a camada curated, tamb√©m conhecida como gold, onde ficam armazenadas as tabelas anal√≠ticas, tamb√©m em formato delta, j√° transformadas conforme os requisitos da √°rea de neg√≥cio, para serem usadas pelos analistas de neg√≥cios
  
    ![bucket curated](https://github.com/Priscaruso/Bookclub_project/assets/83982164/8c53685d-6385-4d03-b157-7a1c0f0c62f4)


### 4¬™ fase - Migra√ß√£o dos dados
Para que as transforma√ß√µes e an√°lises dos dados possam ser realizadas, os dados necessitam estar armazenados em um local apropriado
para isso e que tenha condi√ß√µes de receber constantemente novos dados, ou seja, seja escal√°vel. Assim, o DataLake √© o local ideal
para realizar o armazenamento desses dados. Para que isso seja poss√≠vel, √© preciso realizar a migra√ß√£o dos dados brutos contidos no RDS para a camada raw do DataLake (bucket raw-bookclub). Nesse projeto √© usado o Database Migration Service da AWS, que apesar de ter um custo mais elevado quando se tem um grande volume de dados, √© f√°cil de usar e realiza a migra√ß√£o de forma mais r√°pida. A migra√ß√£o √© feita de acordo com os seguintes passos:
  * Criar uma inst√¢ncia de replica√ß√£o no DMS (dms-instance-01)

    A inst√¢ncia √© criada na mesma regi√£o dos demais servi√ßos, Oregon (us-west) e usando uma m√°quina dms.t3.micro.
    ![inst√¢ncia de replica√ß√£o DMS](https://github.com/Priscaruso/Bookclub_project/assets/83982164/bf598a55-b25e-4fc4-a981-487922842d28)

  
  * Criar os endpoints de origem (rds-source-postgresql), que conecta o DMS com o RDS, e de destino (s3-target-datalake), que conecta o DMS com o Datalake S3
  
    ![endpoints](https://github.com/Priscaruso/Bookclub_project/assets/83982164/aa964165-ecc1-4d90-bdf7-94e1937d49b3)

  
  * Testar os endspoints para verificar se a conex√£o entre o RDS, o DMS e o S3 est√° funcionando corretamente
    
  
  * Criar uma tarefa de migra√ß√£o (task-01) usando a inst√¢ncia de replica√ß√£o e os endpoints gerados
  
    ![tarefa de migra√ß√£o parte 2](https://github.com/Priscaruso/Bookclub_project/assets/83982164/5ba7ca5c-df11-4a1d-b0dc-bbce689581b1)


### 5¬™ fase - Processamento dos dados
Nesta etapa √© utilizado o EMR (Elastic Map Reduce) da AWS para realizar o processamento dos dados, usando uma aplica√ß√£o Spark, que possibilita o processamento de grande volume de dados de forma mais eficiente. A op√ß√£o por usar o EMR, √© que ele √© um cluster (uma m√°quina EC2), que vem com as bibliotecas necess√°rias j√° instaladas o que facilita e economiza tempo, al√©m de s√≥ cobrar pelo tempo de uso da m√°quina, podendo processar a quantidade de dados que desejar nesse per√≠odo, sem ter aumento de custo por conta disso. 
O processamento dos dados consiste nos seguintes passos:
  * Criar um cluster EMR contendo somente a aplica√ß√£o Spark vers√£o 3.3.0
  
    ![cluster EMR - vers√£o editada](https://github.com/Priscaruso/Bookclub_project/assets/83982164/89a21f7b-3fcf-4ef6-99a4-f2888e5f1515)

  * Criar um job spark por meio de uma aplica√ß√£o pyspark
  
    O job spark √© uma tarefa que ser√° executada pelo cluster EMR, no caso, a aplica√ß√£o pyspark criada. A aplica√ß√£o √© respons√°vel por fazer as transforma√ß√µes desejadas nos dados e inser√≠-los na camada processed (bucket processed-bookclub). Ela tamb√©m gera as tabelas anal√≠ticas conforme os requisitos solicitados pela √°rea de neg√≥cios e carrega-as tanto na camada curated (bucket curated-bookclub) do datalake como no Data Warehouse, que √© o Amazon Redshift. Essa aplica√ß√£o pyspark de nome [job_spark_app_emr_redshift.py](https://github.com/Priscaruso/Bookclub_project/blob/main/processing/job_spark_app_emr_redshift.py) pode ser encontrada dentro da pasta processing deste reposit√≥rio.
   


### 6¬™ fase - Constru√ß√£o do Data Warehouse 

### 7¬™ fase - Consulta dos dados 

### 8¬™ fase - Visualiza√ß√£o dos dados


## Pr√≥ximos passos
Com o desejo de evoluir o projeto e torn√°-lo ainda mais completo, quero incluir nos pr√≥ximos meses as seguintes funcionalidades:
  * Orquestrar o pipeline usando o Airflow
  * Criar toda infraestrutura como c√≥digo (IaC) usando o Terraform

------------------------------------------------------------------------------------------------------



