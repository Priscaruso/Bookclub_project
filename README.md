# Book Club Project üìö 

üáßüá∑

A Book Club √© uma startup fict√≠cia de troca de livros, cujo modelo de neg√≥cio funciona com base na troca de livros pelos usu√°rios. Cada livro cadastrado pelo 
usu√°rio d√° direito √† uma troca, mas o usu√°rio tamb√©m pode comprar o livro, caso ele n√£o queira oferecer outro livro em troca. 

Uma das ferramentas mais importantes para que esse modelo de neg√≥cio rentabilize, √© a recomenda√ß√£o. Uma excelente recomenda√ß√£o aumenta o volume de 
trocas e vendas no site. No entanto, a empresa n√£o coleta e nem armazena os livros enviados pelos usu√°rios em um banco de dados. Os livros s√£o apenas enviados pelos usu√°rios atrav√©s de um bot√£o "Fazer Upload", ficando vis√≠veis na plataforma, junto com suas estrelas, que representam o quanto os usu√°rios gostaram do livro.

## T√≥picos do projeto

:small_blue_diamond: [Objetivo](#objetivo)

:small_blue_diamond: [Arquitetura dos dados](#arquitetura-dos-dados)

:small_blue_diamond: [Pr√©-requisitos](#pr√©-requisitos)

:small_blue_diamond: [Fases do Projeto](#fases-do-projeto)

:small_blue_diamond: [Dificuldades encontradas durante o projeto](#dificuldades-encontradas-durante-o-projeto)

:small_blue_diamond: [Pr√≥ximos passos](#pr√≥ximos-passos)


## Objetivo
Criar uma solu√ß√£o para coletar, armazenar e analisar os dados da plataforma web fict√≠cia de troca de livros usando os servi√ßos da nuvem AWS.

Dados desejados:

* Nome do livro
* Categoria do livro
* N√∫mero de estrelas
* Pre√ßo
* Disponibilidade do livro: em estoque ou n√£o

Link da plataforma: http://books.toscrape.com


## Arquitetura dos dados
A arquitetura do projeto utiliza os servi√ßos da nuvem AWS aplicados √† engenharia de dados, conforme mostra a figura a seguir:

![Arquitetura dos dados](https://user-images.githubusercontent.com/83982164/235518227-0165070a-5b47-459f-b75b-dd99b56203b3.png)



## Pr√©-requisitos
Para executar esse projeto √© necess√°rio criar um ambiente virtual, instalar os pacotes Python contidos no arquivo [requirements.txt](https://github.com/Priscaruso/Bookclub_project/blob/main/requirements.txt) e ter uma conta na [AWS](https://aws.amazon.com/pt/console/).

Esse projeto est√° dividido em fases de constru√ß√£o.

## Fases do Projeto

### 1¬™ fase - Coleta dos dados

Cria√ß√£o do script python [bookclub_webscraper.py](https://github.com/Priscaruso/Bookclub_project/blob/main/bookclub_webscraper.py) que define as fun√ß√µes para a coleta dos dados da plataforma.

O funcionamento do script acontece da seguinte forma:
* Fun√ß√£o _"get_book_links"_ navega por todas as p√°ginas da plataforma, coletando e armazenando os links da p√°gina de cada livro encontrado dentro de uma lista usando as bibliotecas Selenium e BeautifulSoup 
* Fun√ß√£o _"get_book_data"_ navega por cada p√°gina do livro para coletar os dados desejados e armazen√°-los dentro de uma lista


### 2¬™ fase - Armazenamento dos dados

Essa fase √© subdividida em 3 passos:
* Cria√ß√£o do banco de dados

  Constru√ß√£o de uma inst√¢ncia RDS PostgreSQL na nuvem AWS para armazenar os dados operacionais da plataforma, conforme mostra a figura abaixo: 
  
  ![Banco de dados RDS](https://github.com/Priscaruso/Bookclub_project/assets/83982164/3212367c-f903-4a63-bef8-c0bcbb9338b7)

   A regi√£o selecionada foi a de Oregon nos EUA (us-west-2) e a m√°quina foi a db.t3.micro por quest√µes de custo e para atender ao volume de dados necess√°rio para esse projeto.
 
* Modelagem dos dados

  Cria√ß√£o do script python [data_model.py] que cria as seguintes fun√ß√µes:
  * Fun√ß√£o _check_if_valid_data_ que verifica se os dados coletados apresentam dados ausentes ou n√£o
  * Fun√ß√£o _connect_db_ que cria uma conex√£o com o banco bookclub no RDS usando a url do host, o nome do banco, o usu√°rio padr√£o do banco, a senha de acesso criada e a porta de conex√£o. Tanto o usu√°rio como a senha de acesso s√£o armazenados em um arquivo .env no ambiente virtual criado para o projeto, por meio da biblioteca python dotenv, a fim de garantir a seguran√ßa dos dados.
  * Fun√ß√£o _create_table_ que gera a tabela desejada a partir da execu√ß√£o de uma query, sendo esta tabela armazenada dentro do banco bookclub no RDS. O schema da tabela √© apresentado a seguir:
    
    | Column | Data Type |
    | :---: | :---: |
    | id | int autoincremental sequence, PRIMARY KEY |
    | name | varchar(250) |
    | category | varchar(20) |
    | stars | varchar(5) |
    | price | float |
    | availability | varchar(10) |
    
  
 
* Inser√ß√£o dos dados no banco

  Dentro do script python [data_model.py]() s√£o criadas as seguintes fun√ß√µes:
    * Fun√ß√£o _insert_data_ para inserir os dados no banco a partir de uma query e dos valores das vari√°veis armazenados por meio de uma tupla
    * Fun√ß√£o _consulta_db_ que permite realizar consulta dos dados no banco por meio de uma query e retorna os dados na forma de lista

Ap√≥s a cria√ß√£o das fun√ß√µes, √© configurado os par√¢metros de navega√ß√£o para a biblioteca selenium, √© criado o objeto de navega√ß√£o, √© chamada a fun√ß√£o _get_book_links_ e _get_book_data_ para coletar os dados da plataforma. Os dados s√£o transformados em dataframe, √© chamada a fun√ß√£o _create_table_ para criar a tabela books dentro do RDS, s√£o realizadas as transforma√ß√µes necess√°rias nos dados (remo√ß√£o de ap√≥strofes e sinais de % em strings) e feita a inser√ß√£o desses dados na tabela books dentro do banco bookclub no RDS.


### 3¬™ fase - Constru√ß√£o do Datalake
Nesta fase √© criado o Datalake usando o Amazon S3, que √© um reposit√≥rio de objetos. 
S√£o criadas tr√™s camadas para armazenar os dados:
  * raw-bookclub - √© a camada raw, tamb√©m conhecida como bronze, onde ficam os dados brutos coletados no seu formato original (geralmente CSV)
  
     ![bucket raw](https://github.com/Priscaruso/Bookclub_project/assets/83982164/e9f615da-84ab-4862-8009-c3b3974541a9)

  
  * processed-bookclub  - √© a camada processed, tamb√©m conhecida como silver, onde ficam os dados que foram transformados para o formato delta, mais otimizado para serem usados na nuvem pelos cientistas e analistas de dados
  
    ![bucket processed](https://github.com/Priscaruso/Bookclub_project/assets/83982164/80bec75d-8486-4ae6-9f6a-9de3302586b8)

  * curated-bookclub - √© a camada curated, tamb√©m conhecida como gold, onde ficam armazenadas as tabelas anal√≠ticas, tamb√©m em formato delta, j√° transformadas conforme os requisitos da √°rea de neg√≥cio, para serem usadas pelos analistas de neg√≥cios
  
    ![bucket curated](https://github.com/Priscaruso/Bookclub_project/assets/83982164/8c53685d-6385-4d03-b157-7a1c0f0c62f4)


### 4¬™ fase - Migra√ß√£o dos dados
Para que as transforma√ß√µes e an√°lises dos dados possam ser realizadas, os dados necessitam estar armazenados em um local apropriado
para isso e que tenha condi√ß√µes de receber constantemente novos dados, ou seja, seja escal√°vel. Assim, o DataLake √© o local ideal
para realizar o armazenamento desses dados. Para que isso seja poss√≠vel, √© preciso realizar a migra√ß√£o dos dados brutos contidos no RDS para a camada raw do DataLake (bucket raw-bookclub). Nesse projeto √© usado o Database Migration Service (DMS) da AWS, que pode realizar a migra√ß√£o de banco de dados do ambiente on-premise para a nuvem ou da pr√≥pria nuvem para outro local na nuvem, gerenciar a migra√ß√£o tanto de dados full como de dados incrementais por meio do Change Data Capture (CDC). Tem um custo razo√°vel, √© f√°cil de usar e realiza a migra√ß√£o de forma mais r√°pida. A migra√ß√£o √© feita de acordo com os seguintes passos:
  * Criar uma inst√¢ncia de replica√ß√£o no DMS

    A inst√¢ncia √© criada com o nome _dms-instance-01_ na mesma regi√£o dos demais servi√ßos, Oregon (us-west-2), com uma engine na vers√£o 3.4.6, uma m√°quina dms.t3.micro, que tem uma mem√≥ria menor, e no ambiente de dev/teste. O ideal √© selecionar uma m√©moria que corresponda, no m√≠nimo, ao tamanho de mem√≥ria do banco de dados de origem para que n√£o haja problemas de mem√≥ria. Em rela√ß√£o ao espa√ßo de armazenamento, deve ter no m√≠nimo metade do tamanho do banco de dados de origem. Essa inst√¢ncia √© respons√°vel pelo gerenciamento da tarefa de migra√ß√£o.
    
    ![inst√¢ncia de replica√ß√£o DMS](https://github.com/Priscaruso/Bookclub_project/assets/83982164/bf598a55-b25e-4fc4-a981-487922842d28)

  * Criar os endpoints de origem (rds-source-postgresql), que conecta o DMS com o RDS, e de destino (s3-target-datalake), que conecta o DMS com o Datalake S3
  
    ![endpoints](https://github.com/Priscaruso/Bookclub_project/assets/83982164/aa964165-ecc1-4d90-bdf7-94e1937d49b3)

  
  * Testar os endspoints para verificar se a conex√£o entre o RDS, o DMS e o S3 est√° funcionando corretamente
    
  
  * Criar uma tarefa de migra√ß√£o (task-01) usando a inst√¢ncia de replica√ß√£o e os endpoints gerados
  
    A tarefa √© respons√°vel por fazer a conex√£o entre os endpoints, ou seja, o banco de dados de origem e de destino, e √© executada pela inst√¢ncia de replica√ß√£o.
  
  ![tarefa de migra√ß√£o parte 2](https://github.com/Priscaruso/Bookclub_project/assets/83982164/5ba7ca5c-df11-4a1d-b0dc-bbce689581b1)
    
  Ap√≥s a conclus√£o da tarefa de migra√ß√£o, os dados ter√£o sido movidos para o bucket raw-bookclub do Datalake.


  ### 5¬™ fase - Constru√ß√£o do Data Warehouse 
O Data Warehouse √© o armaz√©m de dados anal√≠ticos, onde os analistas de neg√≥cios conseguem obter insights por meio de consultas SQL e tamb√©m servir os dados para as ferramentas anal√≠ticas de visualiza√ß√£o gerarem dashboards, que permitem a eles tomar melhores decis√µes. O Data Warehouse utilizado nesse projeto √© o Amazon Redshift, uma das solu√ß√µes mais usadas no mercado, que armazena as tabelas anal√≠ticas geradas de acordo com as regras de neg√≥cios na etapa de processamento dos dados. O Redshift possui um cluster com v√°rios n√≥s (m√°quinas), o que permite escalar a performance das consultas, al√©m de possuir diversos outros recursos para escabilidade e alta performance.
Para construir o Data Warehouse, precisa-se criar um cluster, conforme os passos abaixo:
  * definir o nome do cluster, no caso redshift-cluster-1
  * selecionar um n√≥ (servidor) do tipo dc2.large (n√≠vel gratuito/free-tier)
  * definir um usu√°rio e uma senha para acesso ao banco de dados que ser√° criado dentro do cluster
A figura abaixo mostra o cluster gerado:

  ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/bb0494fb-2889-4dcf-8890-727ee2771694)

O acesso ao Redshift se d√° atrav√©s do editor de consultas v2 (query editor v2) no qual pode-se realizar consultas SQL nas tabelas anal√≠ticas geradas atrav√©s da aplica√ß√£o pyspark no processamento dos dados.


### 6¬™ fase - Processamento dos dados
Nesta etapa foi utilizado o EMR (Elastic Map Reduce) da AWS para realizar o processamento dos dados, usando uma aplica√ß√£o Spark, que possibilita o processamento de grande volume de dados de forma mais eficiente. A op√ß√£o por usar o EMR, √© que ele √© um cluster (uma m√°quina EC2), que pode vir instalado com diversas aplica√ß√µes open source do ecossistema Hadoop e tem as bibliotecas necess√°rias (como as para o formato Delta) e os JARS necess√°rios para trabalhar com Redshift j√° instalados, o que facilita e economiza tempo. Al√©m disso, ele s√≥ cobra pelo tempo de uso da m√°quina, podendo processar a quantidade de dados que desejar nesse per√≠odo, sem ter aumento de custo por conta disso. 
O processamento dos dados consiste nos seguintes passos:
  * Criar um cluster EMR contendo somente a aplica√ß√£o Spark vers√£o 3.3.0
    
    Foi usado o EMR vers√£o 6.9.0 que j√° vem com as bibliotecas Delta e m√°quinas EC2 do tipo m4.large, que tem menor custo e menor recurso computacional, mas atende ao objetivo do projeto. Durante a cria√ß√£o do cluster, √© necess√°rio gerar um par de chaves SSH no formato PEM para acessar as m√°quinas EC2 do cluster EMR. Al√©m disso, tamb√©m √© necess√°rio criar as roles de acesso a servi√ßos da AWS (EMR_DefaultRole_V2), que permitem o acesso do EMR ao S3 para armazenar os dados processados, e de acesso a inst√¢ncias EC2 (EMR_EC2_DefaultRole), para provisionar as m√°quinas do cluster.
  
    ![cluster EMR - vers√£o editada](https://github.com/Priscaruso/Bookclub_project/assets/83982164/89a21f7b-3fcf-4ef6-99a4-f2888e5f1515)

  * Criar um job spark por meio de uma aplica√ß√£o pyspark
  
    O job spark √© uma tarefa que ser√° executada pelo cluster EMR, no caso, a aplica√ß√£o pyspark criada. A aplica√ß√£o √© respons√°vel por fazer as transforma√ß√µes desejadas nos dados e inser√≠-los na camada processed (bucket processed-bookclub). Ela tamb√©m gera as tabelas anal√≠ticas conforme os requisitos solicitados pela √°rea de neg√≥cios e carrega-as tanto na camada curated (bucket curated-bookclub) do datalake como no Data Warehouse, que √© o Amazon Redshift. Essa aplica√ß√£o pyspark de nome [job_spark_app_emr_redshift.py](https://github.com/Priscaruso/Bookclub_project/blob/main/processing/job_spark_app_emr_redshift.py) pode ser encontrada dentro da pasta processing deste reposit√≥rio.
   
Para executar o job spark necessita-se:
  * habilitar a permiss√£o de execu√ß√£o para o propriet√°rio do arquivo do par de chaves SSH: `chmod 400 local_das_chaves`, onde local_das_chaves √© a pasta onde o par de chaves SSH foi salvo
  * mover a aplica√ß√£o criada para dentro do diret√≥rio padr√£o do Hadoop no cluster EMR, conforme o comando `scp -i local_das_chaves job-spark-app-emr-redshift.py hadoop@url_do_servidor:/home/hadoop/`, onde url_do_servidor √© o link do DNS p√∫blico do n√≥ prim√°rio (servidor master do EMR) localizado na console da AWS a partir das informa√ß√µes do cluster-bookclub
  * conectar remotamente no servidor master usando ssh: `ssh -i local_das_chaves hadoop@url_do_servidor`
  * executar o comando spark-submit para rodar a aplica√ß√£o:
    
    `spark-submit --packages io.delta:delta-core_2.12:2.0.0 --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"  --jars /usr/share/aws/redshift/jdbc/RedshiftJDBC.jar,/usr/share/aws/redshift/spark-redshift/lib/spark-redshift.jar,/usr/share/aws/redshift/spark-redshift/lib/spark-avro.jar,/usr/share/aws/redshift/spark-redshift/lib/minimal-json.jar job-spark-app-emr-redshift.py`
    
O comando acima faz a configura√ß√£o das bibliotecas Delta, para gerar os dados nesse formato, e JARS, para o EMR conseguir trabalhar junto com o Redshift, executando logo em seguida, a aplica√ß√£o.
Ao concluir a execu√ß√£o da aplica√ß√£o, os dados transformados em formato delta estar√£o localizados no bucket processed-bookclub e as tabelas anal√≠ticas _top10_liked_books_ e _top10_prices_ geradas a partir deles, no bucket curated-bookclub e no Redshift.


### 7¬™ fase - Consulta dos dados 
Essa etapa consiste no acesso aos dados anal√≠ticos j√° transformados, de acordo com as regras de neg√≥cios definidas, por meio do Amazon Athena. O Athena √© um servi√ßo serverless (sem necessidade de provisionar servidor) de consultas ad-hoc simples, onde os analistas de neg√≥cios podem rapidamente realizar consultas interativas de forma simples para obter os insights desejados a partir dos dados tratados que est√£o armazenados no S3, no caso desse projeto s√£o os que se encontram na camada curated do Datalake, o bucket curated-bookclub. O custo dele √© por consulta, por quantidade de terabyte escaneado, assim o ideal √© tentar definir um limite usando workgroups (grupos de trabalho) para obter os insights desejados utilizando o menor n√∫mero de consultas poss√≠vel. Para acessar esses dados no Athena, primeiramente, devem ser realizados os seguintes passos:

 * Criar as databases no Glue Data Catalog
    A database √© como se fosse um banco de dados, mas funciona de forma diferente, e √© onde s√£o catalogadas as tabelas (metadados), cujos schemas foram inferidos pelo crawler.
   Para cri√°-la acessa a op√ß√£o _Databases_ e, em seguida, _adicionar nova database_. Na configura√ß√£o, define um nome para a database e seleciona o bot√£o _criar database_. Foram criadas duas databases nesse projeto, a  _books-processed_ e a _books-curated_, conforme mostra a figura abaixo:

    ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/1a5e21ae-b771-4eff-bd06-fa0515a66862)
   
    ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/69206d44-f4fc-4997-9ba8-cc62dc4cb1cc)

  * Criar os crawlers no Glue
    
    O Glue √© um servi√ßo onde pode-se realizar integra√ß√µes de dados, ou seja, transportar os dados de um lugar para o outro, realizar transforma√ß√µes nos dados e catalog√°-los. Para que os dados do S3 possam ser acessados atrav√©s do Amazon Athena, √© necess√°rio usar o Glue para fazer a integra√ß√£o entre esses dois servi√ßos. O primeiro passo para isso √© atrav√©s do Glue crawler. O crawler vai escanear e inferir automaticamente o schema dos dados transformados armazenados no bucket processed-bookclub e curated-bookclub do S3. Ao criar o crawler, configura-se a fonte dos dados, Delta Lake no caso; o caminho onde est√° as tabelas do bucket processed-bookclub e do curated-bookclub do S3; habilita a op√ß√£o criar tabelas nativas para permitir a leitura do formato Delta direto no Athena; um classificador personalizado (algoritmo), que detecta o formato dos dados desejados que ser√° populado no Glue Data Catalog; cria-se um IAM role para o Glue crawler acessar os dados no S3; seleciona a database que vai receber os dados; define que o Glue vai atualizar a defini√ß√£o das tabelas no Data Catalog caso haja alguma altera√ß√£o no schema dos dados no S3; define uma tabela como depreciada no Data Catalog, caso algum objeto seja deletado na fonte S3; especifica a frequ√™ncia que o crawler vai ser executado, que no caso √© on-demand, sendo executado s√≥ quando for desejado. Ap√≥s o t√©rmino da sua cria√ß√£o, o crawler √© executado clicando no bot√£o _run crawler_. S√£o criados dois crawler nesse projeto, o _crawler-processed-bookclub_ e o _crawler-curated-bookclub_, conforme mostra a figura:
    
     ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/b28ffe8a-7765-4064-bf35-8291aa1e0428)

    Ap√≥s o t√©rmino da execu√ß√£o do _crawler-processed-bookclub √© gerada a tabela _books_ na database _books-processed_ e o _crawler-curated-bookclub_ gera as tabelas _top10_liked_books_ e _top10_prices_ na database _books-processed_.

    ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/09791d10-2f5a-4dda-96f9-976950cb3751)

  Para acessar os dados no Athena, basta ir na op√ß√£o _query editor_, selecionar o data source(fonte dos dados) como AWSDataCatalog e a Database desejada, _books-processed_ ou _books-processed_. No menu tables, √© poss√≠vel visualizar as tabelas de cada database. Assim, j√° consegue-se consultar os dados de cada tabela usando SQL. 
  A figura abaixo mostra o resultado da consulta quando se seleciona todas as colunas da tabela _books_:
  
  ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/fee36a24-fcc0-4589-b22c-5e131740935f)
  ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/4f4b08b7-8ead-47ce-9f74-0bf1291a6bdc)

  Os resultados das consultas ao selecionar todas as colunas das tabelas _top10_liked_books_ e _top10_prices_, respectivamente, tamb√©m podem ser visualizados a seguir:

  ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/5a6ad7bf-b613-4161-9664-4f6ed7761ec0)
  ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/41d6ada8-d8a6-407c-af7d-3f7bb3f677e0)
  
  ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/f60b5d33-95e0-4f1b-86f2-c4337a76730b)
  ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/30cc304f-5c98-4262-9376-9833299f6fcd)


### 8¬™ fase - Visualiza√ß√£o dos dados
A √∫ltima etapa do projeto consiste em consumir os dados armazenados no Data Warehouse, o Redshift, por meio de ferramentas de BI para que os analistas de neg√≥cios possam tomar melhores decis√µes por meio da visualiza√ß√£o dos dados. Foram usadas duas ferramentas diferentes, o Power BI e o Amazon Quicksight. A escolha por usar essas duas ferramentas √© porque o Power BI √© a ferramenta de BI mais utilizada no mercado e o Quicksight √© f√°cil de usar, tem um custo mais baixo do que as ferramentas tradicionais como Power BI, al√©m de ser pr√≥pria da AWS, que foi a nuvem usada durante todo o projeto. 
Para acessar os dados do Redshift por meio do Power BI, √© preciso configurar as regras de entrada da VPC onde est√° o Redshift no console da AWS, para permitir que a conex√£o do Quicksight chegue at√© o Redshift. Depois deve-se selecionar a op√ß√£o "Obter dados" e procurar por "Amazon Redshift". Na tela que se abre, configurar a fonte dos dados colocando o link do endpoint do Redshift e o nome do banco de dados onde as tabelas desejadas se encontram, que √© o "dev". Ap√≥s o estabelecimento da conex√£o, basta selecionar as tabelas _top10_liked_books_ e _top10_prices_ na nova tela conforme mostrado abaixo:

  ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/a3f9daf5-f8dd-4ae5-acfe-29b8eabdc7ae)

  Com isso, os analistas de neg√≥cios j√° conseguem criar os dashboards e as visualiza√ß√µes de dados desejadas.

  Para acessar os dados a partir do Amazon Quicksight, deve-se criar uma conta nesse servi√ßo usando o usu√°rio da AWS. Esse servi√ßo √© pago, mas pode test√°-lo gratuitamente por 30 dias. Ap√≥s a cria√ß√£o da conta, assim como foi feito com o Power BI, deve-se configurar a conex√£o para acesso do Quicksight ao Redshift. Essa configura√ß√£o √© feita atrav√©s do Amazon VPC Management Console, por meio do Security Groups (Grupo de Seguran√ßa) no painel de navega√ß√£o conforme os passos abaixo:
  * Um novo Security Group deve ser criado, e nos campos _Tag de Nome_, _Grupo de Nome_ e _Descri√ß√£o_, inserido o nome _Amazon-QuickSight-access_
  * No campo VPC, colocar o ID da VPC do cluster Redshift que ser√° acessado pelo Quicksight e finalizar a cria√ß√£o do Security Group
  * Em regras de entradas do grupo de seguran√ßa, criar uma nova para o Redshift usando a porta TCP do tipo TCP personalizada e n√∫mero de porta igual ao usado para a porta do banco do cluster Redshift
  *  No campo fonte (source), inserir o bloco de endere√ßo CIDR para Regi√£o da AWS onde foi criado a conta do AmazonQuickSight, que nesse caso foi S√£o Paulo, porque essa era a √∫nica regi√£o no momento que tava dispon√≠vel para testar grauitamente a ferramenta
  *  Salvar a regra de entrada e navegar at√© o menu dos clusters do Amazon Redshift Management Console
  *  Exibir as informa√ß√µes do cluster redshift-cluster-1, ir at√© as configura√ß√µes de _Rede e de Seguran√ßa_, clicar em editar e na op√ß√£o VPC Security Group, escolher o _Amazon-QuickSight-access_ e salvar as modifica√ß√µes
  *  Ainda no Amazon Redshift Management Console, ir at√© as configura√ß√µes de _Rede e de Seguran√ßa_ e habilitar o acesso p√∫blico do cluster
    
 Terminadas as configura√ß√µes, o pr√≥ximo passo √© acessar o Quicksight atrav√©s do console da AWS usando a conta criada. Se o Amazon Redshift n√£o estiver configurado como um servi√ßo que o Quicksight pode acessar, ir no menu Seguran√ßa e Permiss√µes, e adicion√°-lo. Em seguida, ir no menu Dataset e selecionar o Redshift como fonte dos dados. Configurar a nova fonte definindo um nome para ela (_top10_liked_books_ e depois _top10_prices_), uma inst√¢ncia de ID (redshift-cluster-1), o tipo de conex√£o (Rede p√∫blica), o nome do banco de dados (dev), nome de usu√°rio e senha do banco. Ap√≥s configurar tudo, selecionar conectar para estabelecer a conex√£o. Em seguida, selecionar o schema e as tabelas desejadas, e o modo de consulta como SPICE, no qual os dados ser√£o armazenados dentro do Quicksight, podendo ser consultados a qualquer momento. Foram criados dois datasets para esse projeto, o _top10_liked_books_ e o _top10_prices_ referentes √†s tabelas de mesmo nome. As visualiza√ß√µes geradas a partir desses datasets encontram-se a seguir:

  ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/04decfd2-d41c-4958-92c2-745362a39e3c)
  
  ![image](https://github.com/Priscaruso/Bookclub_project/assets/83982164/5df45f7b-3f9c-4205-95e9-ecf7df74c7a3)


A primeira visualiza√ß√£o consiste em uma tabela contendo os 10 livros mais curtidos na plataforma pelos usu√°rios e suas respectivas categorias, ou seja, s√£o os livros que tem o maior n√∫mero de estrelas de acordo com a ordem alfab√©tica. Assim, o fato do livro _#HigherSele: Wake Up Your Life. Free Your Soul. Find Your Tribe._ estar na primeira classifica√ß√£o, n√£o significa que ele tem o maior n√∫mero de estrelas entre os 10, pois todos esses livros tem 5 estrelas, e sim que √© o mais curtido segundo a ordem alfab√©tica.

A segunda visualiza√ß√£o mostra os 10 livros mais baratos. O livro _An Abundance of Katherines_ √© o que tem o menor pre√ßo entre os 1000 livros presentes na plataforma.

Essas s√£o apenas algumas das an√°lises poss√≠veis de serem constru√≠das com as ferramentas de BI. A partir delas, os analistas de neg√≥cios j√° conseguem criar dashboards e diversas outras visualiza√ß√µes de dados, conforme desejado.


## Dificuldades encontradas durante o projeto
Durante o desenvolvimento do projeto, foram encontrados alguns problemas e dificuldades que precisaram ser resolvidos para que cada etapa fosse conclu√≠da e assim, atingir o objetivo final do projeto. 
Os problemas foram:
* Erro de conex√£o do webdriver do navegador firefox com o servidor da p√°gina web
* A conex√£o com a p√°gina url da plataforma bookclub n√£o fechava ap√≥s navegar para a p√°gina do √∫ltimo livro desejado
* Erro ao executar a modelagem dos dados, pois o tamanho dos caracteres do campo 'name' da tabela books estava pequeno demais e o campo 'id' da tabela books estava dando como duplicado
* Erro de sintaxe ao tentar inserir strings contendo ap√≥strofes no banco RDS
* Erro de formato n√£o suportado nas strings ao tentar inserir os dados no banco RDS, pois algumas continham % e o python n√£o reconhece isso
* Falha na conex√£o com o banco de dados RDS, pois inicialmente a regra de entrada para acesso p√∫blico ao banco n√£o estava configurada
* Falha no teste de conex√£o do endpoint do DMS com o RDS por o DMS n√£o suportar a vers√£o do postgres usada no RDS (vers√£o 14.6)
* Erro de m√≥dulo n√£o encontrado com o pacote dotenv dentro do cluster EMR ao usar o m√≥dulo na aplica√ß√£o pyspark para salvar as credenciais de acesso, pois o cluster EMR n√£o tinha o dotenv j√° instalado
* Problema de NoneType na execu√ß√£o da aplica√ß√£o Pyspark, pois o dataframe n√£o estava sendo atribu√≠do a nenhuma vari√°vel dentro da fun√ß√£o analytics_table
* Erro ao executar a aplica√ß√£o Pyspark dentro do cluster EMR pois n√£o estava conseguindo inserir os dados no Redshift devido n√£o ter uma regra de entrada configurada para o acesso p√∫blico ao Redshift
* Como conectar o Redshift com o Power BI e com o Amazon Quicksight

Para resolver cada uma delas, foi exigido bastante pesquisa e resili√™ncia, algo que faz parte da rotina de quem trabalha com programa√ß√£o e tecnologia. As dificuldades encontradas foram importantes, pois permitaram a evolu√ß√£o do projeto e o meu desenvolvimento profissional, pois s√≥ com as dificuldades e os erros que podemos aprender e melhorar cada vez mais.


## Pr√≥ximos passos
Com o desejo de evoluir o projeto e torn√°-lo ainda mais completo, quero incluir nos pr√≥ximos meses as seguintes funcionalidades:
  * Orquestrar o pipeline usando o Airflow
  * Criar toda infraestrutura como c√≥digo (IaC) usando o Terraform

------------------------------------------------------------------------------------------------------



